# Metoda nejbližších sousedů
 - Příznaky zapisujeme do vektoru $\textbf{X} = (X_1, \ldots, X_p)^p$ na základě $p$ příznaků.
 - $\mathcal{X}$ - množina obsahující všechny možné hodnoty příznaků $x \in \mathcal{X}$
 - Najdeme $k$ nejbližších sousedů pro predikovaný datový bod $x \in \mathcal{X}$.
 - Predikci zakládáme na známých hodnotách vysvětlované proměnné pro těchto $k$ bodů.
	 - U regrese bereme průměr hodnot
	 - U klasifikace bereme nejčastější hodnotu

## Vzdálenost
Vzdálenost (nebo metrika) na množině $\mathcal{X}$ je funkce  $d: \mathcal{X} \times \mathcal{X} \rightarrow [0, +\infty)$ taková, že pro každé $x, y, z \in \mathcal{X}$ platí
1. $d(x, y) \geq 0$ a $d(x, y) = 0$ právě tehdy, když $x = y$ - pozitivní definitnost
2. $d(x, y) = d(y, x)$  - symetrie
3. $d(x, y) \leq d(x, z) + d(z, y)$ - trojúhelníková nerovnost

Nejběžnější volba : eukleidovská vzdálenost
$$||\textbf{x} - \textbf{y}||_2 = d_2(\textbf{x}, \textbf{y}) = \sqrt{\sum_{i=1}^{p}(x_i - y_i)^2}$$
Pro dva body $\textbf{x} = (x_1, \ldots, x_p)$ a $\textbf{y} = (y_1, \ldots, y_p)$ z $\mathbb{R}^p$.

## Učení vs predikování
V kNN není učení, až u predikce probíhá výpočet. Trénovací data jsou sama o sobě naučeným modelem.

## Hyperparametry v kNN
- Metrika
- $k$ - počet sousedů
- Váhy nejbližších sousedů

### Hyperparametr $k$
- Při menším $k$ je vyšší přeučení.
- S větším $k$ ale zároveň zvyšujeme *bias* - větší ovlivnění vzdálenějšími sousedy.

### Míra vzdálenosti jako hyperparametr
- Nejobvyklejší jsou $L_q$ metriky (Minského $q$-metriky).
- Zobecnění eukleidovské metriky s mocninou na q
$$||\text{x} - \textbf{y}||_q = d_q(\textbf{x}, \textbf{y}) = \sqrt{\sum_{i=1}^{p}|x_i - y_i|^q}$$
	Pro dva body $\textbf{x} = (x_1, \ldots, x_p)$ a $\textbf{y} = (y_1, \ldots, y_p)$ z $\mathbb{R}^p$.

- Pro $q = 1$ se jedná o manhattanskou vzdálenost - $L_1$.
- Eukledieova vzdálenost je $L_2$.
- Občas se používá ještě cosinova vzdálenost $$\frac{x \cdot y}{||x||_2 ||y||_2}$$
- 